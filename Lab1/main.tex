\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{pdflscape}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{systeme}
\usepackage{setspace}
\usepackage{animate}
\usepackage{tikz}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{xr}
\usetikzlibrary{positioning, arrows.meta}
\usepackage[a4paper, margin=1.2in, total={6in, 9in}]{geometry}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage[dvipsnames]{xcolor}
\usepackage{subfiles}
\definecolor{cornflowerblue}{HTML}{6495ED}
\setlength{\parindent}{0pt}       % Ingen indragning
\setlength{\parskip}{0.5\baselineskip} % 80% av radavst√•ndet mellan stycken
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}

\begin{document}
\begin{titlepage}
\begin{center}
KTH Royal Institute of Technology\\
Course: EL2805 - Reinforcement Learning \\
\begin{align*}
    \text{Saga Tran} &\mid 19991105-2182 \\
\text{Anh Do} &\mid 20020416-2317
\end{align*}

\vspace{200pt}
\huge Computer Lab 1\\
\vspace{20pt}
\large

\vspace{210pt}
\large 
Date: 2025/12/05 18.00 \\
\end{center}
\end{titlepage}
\clearpage

\tableofcontents
\clearpage

\section{Problem 1}
    \subsection{Basic Maze}
        \subsubsection{(a)}\label{Problem1a}
        The problem is modeled as a finite horizon Markov Decision Process (MDP) defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$.

            \myparagraph{State Space ($\mathcal{S}$)}
            The state space is defined by the joint position of the Player ($P$) and the Minotaur ($M$) within the grid.
            \begin{equation}
                \mathcal{S} = \{ (p, m) \mid p \in \mathcal{C}, m \in \mathcal{D} \}
            \end{equation}

            where $\mathcal{D}$ represents the set of all discrete cells in the maze grid, while $\mathcal{C}$ represents the set of all discrete non-wall cells in the maze grid (e.g., $p$ and $m$ have each coordinates $(x, y)$ and $\mathcal{C}\subset\mathcal{D}$). 
            The terminal states are:
            \begin{itemize}\label{SpecialStates}
                \item \textbf{Win State:} $s_{win} = \{ (p, m) \in \mathcal{S} \mid p = B, m \neq B \}$. The player reaches exit $B$ without being caught.
                \item \textbf{Loss State:} $s_{loss} = \{ (p, m) \in \mathcal{S} \mid p = m \}$. The Minotaur occupies the same cell as the player (caught).
            \end{itemize}
            Once in a terminal state (e.g., $s_{win}$ or $s_{loss}$) the game transitions to the absorbing state $s_{done}$. 
            This is mainly for handling the accumulation of rewards at $s_{win}$.


            \myparagraph{Action Space ($\mathcal{A}$)}
            At each time step $t$, the player can choose to move to an adjacent cell or stay still.
            \begin{equation}
                \mathcal{A} = \{ \text{Stay, Up, Down, Left, Right} \}
            \end{equation}

            Note: Moves are constrained by the maze walls for the player.

            \myparagraph{Transition Probabilities ($\mathcal{P}$)}
            The state transitions from $s_t = (p_t, m_t)$ to $s_{t+1} = (p_{t+1}, m_{t+1})$ occur simultaneously. The transition probability factorizes as:
            \begin{equation}
                P(s_{t+1} \mid s_t, a_t) = P(p_{t+1} \mid p_t, a_t) \times P(m_{t+1} \mid m_t)
            \end{equation}

            \begin{itemize}
                \item \textbf{Player Dynamics:} Deterministic. $p_{t+1}$ is the result of action $a_t$ applied to $p_t$. If the move hits a wall or is invalid, $p_{t+1} = p_t$.
                \item \textbf{Minotaur Dynamics:} Random Walk. The Minotaur moves to any adjacent cell within the grid boundaries with uniform probability. It cannot stand still and ignores internal walls.
                \begin{equation}
                    P(m_{t+1} \mid m_t) = \frac{1}{|N(m_t)|} \quad \text{if } m_{t+1} \in N(m_t)
                \end{equation}
                where $N(m_t)$ is the set of valid neighbors (Up, Down, Left, Right) for the Minotaur at position $m_t$, restricted only by the exterior borders of the maze.
            \end{itemize}
            
            Observe that some transitions result in a winning or losing state based on the conditions defined in \ref{SpecialStates}. 
            In these cases, if an action leads to multiple winning outcomes, we must sum their probabilities.

            To handle the transitions to the absorbing states correctly, we define:
            \begin{equation}
                P(s_{done} \mid s_t, a_t) = 
                \begin{cases} 
                    1 & \text{if } s_t \in s_{win} \cup s_{loss} \\
                    0 & \text{otherwise}
                \end{cases}
            \end{equation}

            \myparagraph{Reward Function ($\mathcal{R}$)}\label{RewardFunctionBasic}
            To maximize the probability of exiting the maze, we define the reward to be 1 only upon being at the "Win" state, and 0 otherwise. This transforms the expected cumulative reward into the probability of success.

            \begin{equation}
                R(s_t, a_t) = 
                \begin{cases} 
                    1 & \text{if } s_{t} = s_{win} \text{ (Reached B, not eaten)} \\
                    0 & \text{otherwise}
                \end{cases}
            \end{equation}

        \subsubsection{(b)}
        Yes, introducing the possibility for the minotaur to stand still makes the problem more difficult. 
        This is because the agent must now account for an additional action of the minotaur, which increases the complexity of predicting its movements and planning a safe path to the goal. 
        In the original problem, it is clear that we will never be eaten as long as we keep moving, due to the even steps between the player and the minotaur. However, allowing the minotaur to stand still breaks this parity, making it harder to find an optimal policy. In the extended problem, standing still could now become an optimal action in certain situations, further complicating the model.

    \subsection{Dynamic Programming}
        \subsubsection{(c)}
        Using Dynamic Programming, we computed the optimal policy for the basic maze setup.

        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/1c.png}
                \caption{With reward structure (\ref{RewardFunctionBasic})}
                \label{fig:problem1c_true}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/1c2.png}
                \caption{With penalized stepping}
                \label{fig:problem1c_mod}
            \end{subfigure}
            \caption{Optimal policies for the basic maze using Dynamic Programming. 
            The green circle indicates the starting position of the player, while the red cross marks the Minotaur's starting position. The blue triangle points to the exit point B.
            (a) Policy under the standard sparse reward.
            (b) Policy when a step penalty is introduced to discourage loitering.}
            \label{fig:problem1c_combined}
        \end{figure}

        Figure \ref{fig:problem1c_true} shows that Dynamic Programming successfully finds the optimal policy to reach the exit. 
        The model sometimes stops or moves randomly, which might look strange, but it makes sense given the 20-step horizon. 
        The agent only needs 15 steps to exit and 16 steps to claim the reward, so the extra steps do not affect the total reward. 
        The reward function in \ref{RewardFunctionBasic} depends only on reaching the exit and does not penalize time. 
        Therefore, the policy can make unnecessary moves in the beginning as long as it reaches the goal safely.

        In contrast, Figure \ref{fig:problem1c_mod} illustrates the optimal policy when we modify the reward function to penalize each step taken by the player. 
        We observe that the agent now takes a more direct path to the exit, avoiding unnecessary moves and reaching the goal in exactly 15 steps.
                
        \subsubsection{(d)}
        Utilizing the reward structure defined in \ref{RewardFunctionBasic}, the probability of escaping the maze is calculated directly from the value function at the start state at initial time ($t=0$) that was obtained via Dynamic Programming.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{figures/1d.png}
            \caption{Probability of escaping the maze under different time horizons ($T$) using Dynamic Programming. The plot compares the scenarios where the Minotaur can and cannot stand still.}
            \label{fig:problem1d}
        \end{figure}

        As shown in Figure \ref{fig:problem1d}, when the Minotaur cannot stand still, the escape probability reaches exactly 1 for time horizons $T \geq 16$. 
        This sharp jump indicates that a deterministic winning policy exists. 
        The strict movement constraints of the Minotaur allow the agent to exploit even step parity and guarantee an escape given sufficient time.

        In contrast, when the Minotaur is allowed to stand still, the escape probability asymptotically approaches 1 but does not reach certainty within the observed finite horizons. 
        Since the Minotaur retains the option to remain stationary, there always exists a non-zero probability that it will block the exit path for the duration of the episode, making a guaranteed escape impossible within a fixed time frame.
    
        \subsection{Value Iteration}
        \subsubsection{(e)}
        We are given that the agent's life span $L$ follows a geometric distribution with mean $\mu = 30$. 
        The probability of dying at any time step $t$, denoted as $p_{die}$, is derived from the mean:
        \begin{equation}
            \mathbb{E}[L] = \frac{1}{p_{die}} = 30 \implies p_{die} = \frac{1}{30}
        \end{equation}
        Consequently, the probability of surviving a single time step is $\gamma = 1 - p_{die} = \frac{29}{30}$.

        To maximize the probability of exiting the maze alive, we must maximize the probability of reaching the goal state $B$ at some time $T$ multiplied by the probability of surviving the poison until time $T$. 
        In the MDP framework, this survival probability acts identically to a discount factor. 
        Therefore, we modify the MDP from problem \ref{Problem1a} by introducing a discount factor $\gamma < 1$.

        The tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$ remains largely the same, with the following addition:

        \begin{itemize}
            \item \textbf{Discount Factor ($\gamma$):} We set $\gamma = \frac{29}{30} \approx 0.9667$.
        \end{itemize}

        Since the reward is only received once upon exiting (at time $T$), the expected return becomes $\mathbb{E}[\gamma^T \cdot 1]$. 
        Since $\gamma^T$ is exactly the probability of surviving $T$ steps of poison, maximizing this value maximizes the probability of exiting alive. 
        In this case the optimal $T^* = 15$. So the probability of exiting alive with the optimal policy would thus be $\gamma^{T^*} = \left(\frac{29}{30}\right)^{15} \approx 0.6014$. 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{figures/1e.png}
            \caption{Optimal policy for the basic maze with poison using Value Iteration. The green circle indicates the starting position of the player, while the red cross marks the Minotaur's starting position. The blue triangle points to the exit point B.}
            \label{fig:problem1e}
        \end{figure}

        As shown in Figure \ref{fig:problem1e}, the Value Iteration algorithm successfully computes the optimal policy for the environment with poison. 
        We observe that the agent now strictly follows the most direct path to the exit, minimizing the number of steps taken to reduce the cumulative risk of succumbing to the poison.
        This behavior is quantitatively supported by the computed Value function at the start state when $t=0$, which is $0.6014$. 
        This aligns with the theoretical survival probability for the minimum required
        
        \subsubsection{(f)}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{figures/1f.png}
            \caption{Estimated survival probability vs. number of iterations using Value Iteration and Monte Carlo simulation (cumulative sum averaging). The dashed lines represent the theoretical survival probabilities derived from the value function and direct calculation.}
            \label{fig:problem1f}
        \end{figure}

        We can clearly see that this simulation estimate converges to the value function result of $0.6014$, which confirms that our Value Iteration implementation is correct and that the derived optimal policy effectively maximizes the probability of exiting the maze alive.

    \subsection{Additional questions}
        \subsubsection{(g)}
            \myparagraph{On-policy vs. Off-policy Learning}
            The distinction between these learning methods lies in the relationship between the \textit{behavior policy} (used to explore) and the \textit{target policy} (being learned).
            \begin{itemize}
                \item \textbf{On-policy methods} (e.g., SARSA) estimate the value of the policy currently being used by the agent. The update rule depends on the action actually taken:
                \[ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)] \]
                where $a'$ is the action executed by the current policy, the same we used to get $a$.
                
                \item \textbf{Off-policy methods} (e.g., Q-learning) estimate the value of an optimal target policy independently of the agent's actions. The update rule uses the best possible next action:
                \[ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \]
                This allows the agent to learn an optimal strategy even while exploring randomly, which means $a'$ is chosen greedily instead and not by the current policy.
            \end{itemize}

            \myparagraph{Convergence Conditions}
            For both algorithms to converge to $Q^*$ with probability 1, the step sizes $\alpha_t(s,a)$ must satisfy the Robbins-Monro conditions:
            \begin{equation}
                \sum_{t=0}^{\infty} \alpha_t(s, a) = \infty \quad \text{and} \quad \sum_{t=0}^{\infty} \alpha_t^2(s, a) < \infty
            \end{equation}
            Additionally, specific exploration conditions are required:
            \begin{itemize}
                \item \textbf{SARSA (On-policy)} Requires all state-action pairs must be visited infinitely often, and the policy must become greedy in the limit (e.g., $\epsilon \to 0$).
                \item \textbf{Q-learning (Off-policy)} Requires only that all state-action pairs are visited infinitely often. The behavior policy does not need to become greedy.
            \end{itemize}
    
        \subsubsection{(h)}
        To accommodate the new requirements (keys, smarter Minotaur, and poison), we modify the MDP tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ as follows:

            \myparagraph{State Space Expansion} 
            We introduce a binary variable $k \in \{0, 1\}$ to track possession of the keys. 
            The new state is $s = (p, m, k)$, where $p$ is the player's position, $m$ is the Minotaur's position, and $k=1$ implies the key has been retrieved from C.
            We also need to modify the winning state to require the key aswell as not being eaten:
            \begin{equation}
                s_{win} = \{ (p, m, k) \in \mathcal{S} \mid p = B, m \neq B, k = 1 \}
            \end{equation}

            \myparagraph{Discount Factor (Poison)} 
            To model the geometric life expectancy of 50 steps, we set the discount factor $\gamma$ to the survival probability:
            \begin{equation}
                \gamma = 1 - \frac{1}{50} = \frac{49}{50} = 0.98
            \end{equation}

            \myparagraph{Transition Probabilities} 
            The transition $P(s_{t+1}|s_t,a_t)$ now accounts for the Minotaur's aggressive behavior. 
            Let $\mathcal{N}(m_t)$ be the set of valid neighbors for the Minotaur. 
            Let $\mathcal{N}^*(m_t, p_t) \subset \mathcal{N}(m_t)$ be the subset of neighbors that minimizes the Manhattan distance to the player $p_t$, which is given by:
            \begin{equation}
                m_{t+1} \in \mathcal{N}^*(m_t, p_t), \quad d(m_{t+1}, p_t) = \min_{m \in \mathcal{N}(m_t)} d(m, p_t)
            \end{equation}
            
            The probability of the Minotaur moving to a specific neighbor $m' \in \mathcal{N}(m)$ is given by:
            \begin{equation}
                P(m_{t+1} \mid m_t, p_t) = \frac{0.65}{|\mathcal{N}(m_t)|} + \mathbb{I}(m_{t+1} \in \mathcal{N}^*(m_t, p_t)) \cdot \frac{0.35}{|\mathcal{N}^*(m_t, p_t)|}
            \end{equation}
            Here, the first term represents the random behavior (uniform over all directions), and the second term adds probability mass to the optimal "chasing" moves. 
            
            The key transitions to 1 deterministically as long as $s = (p = C, m \neq C, k = 0)$ for the rest it is inherited from the previous state:
            \begin{equation}
                k_{t+1} = 
                \begin{cases} 
                    1 & \text{if } p_t = C, \, m_t \neq C, \, k_t = 0 \\
                    k_t & \text{otherwise}
                \end{cases}
                \label{eq:KeyTransition}
            \end{equation}

            \myparagraph{Reward Expansion}
            Although not explicitly needed, expanding the rewards to include key retrieval could help convergence. However, to keep in line with the reward structure in \ref{RewardFunctionBasic}, we simply set this term to 0.

    \subsection{Q-Learning and Sarsa}
        \subsubsection{(i) BOUNS}
            \myparagraph{1)}
            \begin{algorithm}
            \caption{Q-learning}
            \begin{algorithmic}[1]
            \Require Environment $\mathcal{E}$, Start State $s_{start}$, Discount $\gamma$, Episodes $K$
            \State Initialize $Q(s, a) \sim U(0,1)$ for all $s \in \mathcal{S}, a \in \mathcal{A}$
            \State Set $Q(\text{absorbing}, \cdot) \gets 0$
            \State Initialize visit counters $N(s, a) \gets 0$ for all $s, a$
            \State Initialize array $V_{starts}$ of size $K$

            \For{episode $k = 1, 2, \dots, K$}
                \State $s \gets s_{start}$
                \State $\varepsilon \gets \epsilon(k)$ \Comment{Get current exploration rate}
                
                \While{$s$ is not absorbing}
                    \State \textbf{Select action} $a$:
                    \If{$\text{random}() < \varepsilon$}
                        \State $a \gets \text{random action from } \mathcal{A}$
                    \Else
                        \State $a \gets \arg\max_{a'} Q(s, a')$
                    \EndIf
                    
                    \State \textbf{Observe} reward $r(s,a)$ and next state $s'$
                    \State $N(s, a) \gets N(s, a) + 1$
                    \State $\alpha \gets \text{learning\_rate}(N(s, a))$
                    
                    \State \textbf{Update Q-value}:
                    \State $Q(s, a) \gets Q(s, a) + \alpha \cdot \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$
                    
                    \State $s \gets s'$
                \EndWhile
                \State $V_{starts}[k] \gets \max_{a} Q(s_{start}, a)$
            \EndFor
            \State \Return $Q, N, V_{starts}$
            \end{algorithmic}
            \label{alg:Qlearning}
            \end{algorithm}

            \myparagraph{2)}
            \begin{figure}[H]
                \centering
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1i2Zero.png}
                    \caption{Q initialized to zero}
                    \label{fig:problem1i2Zero}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1i2Ones.png}
                    \caption{Q initialized to one}
                    \label{fig:problem1i2Ones}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.8\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1i2Rand.png}
                    \caption{Q initialized uniformly}
                    \label{fig:problem1i2Rand}
                \end{subfigure}
                \caption{Convergence of the value function for different $Q$-table initializations with diffrent $\epsilon$, where $\alpha=2/3$.}
                \label{fig:problem1i2}
            \end{figure}
            
            It is evident from Figure \ref{fig:problem1i2} that initializing the $Q$-table uniformly between 0 and 1 leads to the fastest convergence of the value function at the start state. 
            Random initialization within a reasonable bound encourages exploration by introducing stochastic optimism, as some values start high by chance without being uniformly overly optimistic. 
            This allows the model to converge faster than when initialized to all ones, where the agent must systematically lower the $Q$-values for almost every state-action pair, requiring significantly more updates. 
            Conversely, initializing to zero results in a pessimistic model that relies entirely on the $\epsilon$-greedy mechanism for exploration, failing to naturally incentivize visiting unknown states and in this case not even converging within the definied episodes.

            We can also observe that a higher $\epsilon$ (more exploration) generally leads slower convergence across all initializations. 
            This is because excessive exploration prevents the agent from sufficiently exploiting learned knowledge to refine the $Q$-values.
            
            \myparagraph{3)}
            \begin{figure}[H]
                \centering
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1i3Zero.png}
                    \caption{Q initialized to zero}
                    \label{fig:problem1i3Zero}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1i3Ones.png}
                    \caption{Q initialized to one}
                    \label{fig:problem1i3Ones}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.8\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1i3Rand.png}
                    \caption{Q initialized uniformly}
                    \label{fig:problem1i3Rand}
                \end{subfigure}
                \caption{Convergence of the value function for different $Q$-table initializations with different $\alpha$ where $\epsilon=0.1$.}
                \label{fig:problem1i3}
            \end{figure}
            Figure \ref{fig:problem1i3} confirms that uniform random initialization yields the fastest convergence. 
            We observe that a learning rate parameter of $\alpha = 0.51$ outperforms $\alpha = 2/3$, as the latter decays too quickly to sustain learning. 
            Optimistic initialization (to one) performs better here than in previous experiments because the high initial learning rate rapidly corrects the over-estimated values. 
            Although the random initialization suffers a slight initial over-correction, it ultimately achieves higher precision (5 correct decimals vs. 4) after 200,000 episodes. 
            
            Note that the episode count was increased to 200,000 to highlight these asymptotic differences.
        
        \subsubsection{(j) BOUNS}
            \myparagraph{1)}
            The only thing that changes in the SARSA algorithm compared to Q-learning is the update rule.
            Instead of using the maximum $Q$-value of the next state, we use the $Q$-value of the next action $a'$, selected according to the current policy.
            To be more precise, instead of the following from Algorithm \ref{alg:Qlearning}:
            \begin{algorithmic}[1]
                \State $Q(s, a) \gets Q(s, a) + \alpha \cdot \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$
            \end{algorithmic}
            We now have:
            \begin{algorithmic}[1]
                \State \textbf{Select next action} $a'$:
                    \If{$\text{random}() < \varepsilon$}
                        \State $a' \gets \text{random action from } \mathcal{A}$
                    \Else
                        \State $a' \gets \arg\max_{a'} Q(s', a')$
                    \EndIf
                \State $Q(s, a) \gets Q(s, a) + \alpha \cdot \left( r + \gamma Q(s', a') - Q(s, a) \right)$
            \end{algorithmic}

            \myparagraph{2)}
            \begin{figure}[H]
                \centering
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1j2Zero.png}
                    \caption{Q initialized to zero}
                    \label{fig:problem1j2Zero}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1j2Ones.png}
                    \caption{Q initialized to one}
                    \label{fig:problem1j2Ones}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.8\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1j2Rand.png}
                    \caption{Q initialized uniformly}
                    \label{fig:problem1j2Rand}
                \end{subfigure}
                \caption{Convergence of the value function for different $Q$-table initializations with diffrent $\epsilon$, where $\alpha=2/3$.}
                \label{fig:problem1j2}
            \end{figure}
            A fixed $\epsilon=0.2$ hinders convergence (Figure \ref{fig:problem1j2}) because SARSA updates $Q$-values using the actual next action, including suboptimal exploratory moves. 
            This prevents the value estimates from stabilizing on the theoretical optimum, as the agent effectively learns the value of a policy that acts randomly $\epsilon$ of the time. 
            Random initialization remains superior to other methods, though no configuration reaches the optimal value within 1,000,000 episodes due to the fixed exploration rate.

            \myparagraph{3)}
            \begin{figure}[H]
                \centering
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1j3Zero.png}
                    \caption{Q initialized to zero}
                    \label{fig:problem1j3Zero}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1j3Ones.png}
                    \caption{Q initialized to one}
                    \label{fig:problem1j3Ones}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.8\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{figures/1j3Rand.png}
                    \caption{Q initialized uniformly}
                    \label{fig:problem1j3Rand}
                \end{subfigure}
                \caption{Convergence of the value function for different $Q$-table initializations with different $\alpha$ where $\epsilon=0.1$.}
                \label{fig:problem1j3}
            \end{figure}
            With a decaying $\epsilon$, the model finally converges to the optimal value (Figure \ref{fig:problem1j3Ones}).
            In this setting, optimistic initialization (ones) outperforms random initialization because the high initial learning rate rapidly corrects the over-estimations. 
            regarding parameter tuning, maintaining $\alpha > \delta$ is not necessarily beneficial when convergence is smooth, as a faster decaying $\epsilon$ encourages the model to exploit the optimal policy sooner. 
            However, for random initialization (Figure \ref{fig:problem1j3Rand}), a higher learning rate proves superior, as it allows for significant updates later in training, helping the agent escape local optima caused by insufficient early exploration.
    
            We iterated for 1,000,000 episodes to ensure convergence across all configurations. To remain in line with the problem formulation, we have also marked the 50,000-episode threshold for reference.
       
        \subsubsection{(k) BOUNS}
            
    \clearpage
    \newpage
\section{Problem 2}
\subsubsection{(a)}
\subsubsection{(b)}
\subsubsection{(c)}
\subsubsection{(d)}
\subsubsection{(e)}
\subsubsection{(f)}
\end{document}